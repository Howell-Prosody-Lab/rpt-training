{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f65e50d",
   "metadata": {},
   "source": [
    "# Dual Report: F1, Recall, Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5200d6",
   "metadata": {},
   "source": [
    "# Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75aeb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.3071 - val_accuracy: 0.9986 - val_loss: 0.1648\n",
      "Epoch 2/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.1411 - val_accuracy: 0.9986 - val_loss: 0.1334\n",
      "Epoch 3/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.1272 - val_accuracy: 0.9981 - val_loss: 0.1253\n",
      "Epoch 4/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.1201 - val_accuracy: 0.9967 - val_loss: 0.1202\n",
      "Epoch 5/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9972 - loss: 0.1151 - val_accuracy: 0.9952 - val_loss: 0.1180\n",
      "Epoch 6/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.1134 - val_accuracy: 0.9950 - val_loss: 0.1190\n",
      "Epoch 7/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9969 - loss: 0.1113 - val_accuracy: 0.9960 - val_loss: 0.1130\n",
      "Epoch 8/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9969 - loss: 0.1098 - val_accuracy: 0.9960 - val_loss: 0.1119\n",
      "Epoch 9/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.1092 - val_accuracy: 0.9960 - val_loss: 0.1114\n",
      "Epoch 10/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.1089 - val_accuracy: 0.9948 - val_loss: 0.1121\n",
      "Epoch 11/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9961 - loss: 0.1081 - val_accuracy: 0.9938 - val_loss: 0.1088\n",
      "Epoch 12/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.1082 - val_accuracy: 0.9902 - val_loss: 0.1156\n",
      "Epoch 13/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9954 - loss: 0.1079 - val_accuracy: 0.9933 - val_loss: 0.1103\n",
      "Epoch 14/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.1073 - val_accuracy: 0.9955 - val_loss: 0.1082\n",
      "Epoch 15/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.1071 - val_accuracy: 0.9947 - val_loss: 0.1078\n",
      "Epoch 16/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.1065 - val_accuracy: 0.9938 - val_loss: 0.1081\n",
      "Epoch 17/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.1067 - val_accuracy: 0.9905 - val_loss: 0.1114\n",
      "Epoch 18/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9954 - loss: 0.1063 - val_accuracy: 0.9950 - val_loss: 0.1095\n",
      "Epoch 19/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9952 - loss: 0.1062 - val_accuracy: 0.9959 - val_loss: 0.1078\n",
      "Epoch 20/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9954 - loss: 0.1064 - val_accuracy: 0.9957 - val_loss: 0.1090\n",
      "Epoch 1/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.1062 - val_accuracy: 0.9952 - val_loss: 0.1069\n",
      "Epoch 2/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.1053 - val_accuracy: 0.9938 - val_loss: 0.1074\n",
      "Epoch 3/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.1052 - val_accuracy: 0.9941 - val_loss: 0.1072\n",
      "Epoch 4/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.1056 - val_accuracy: 0.9943 - val_loss: 0.1080\n",
      "Epoch 5/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.1051 - val_accuracy: 0.9962 - val_loss: 0.1080\n",
      "Epoch 6/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9950 - loss: 0.1052 - val_accuracy: 0.9933 - val_loss: 0.1085\n",
      "Epoch 7/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.1051 - val_accuracy: 0.9952 - val_loss: 0.1064\n",
      "Epoch 8/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.1048 - val_accuracy: 0.9945 - val_loss: 0.1078\n",
      "Epoch 9/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9952 - loss: 0.1046 - val_accuracy: 0.9943 - val_loss: 0.1140\n",
      "Epoch 10/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.1049 - val_accuracy: 0.9953 - val_loss: 0.1124\n",
      "Epoch 11/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.1050 - val_accuracy: 0.9940 - val_loss: 0.1072\n",
      "Epoch 12/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.1046 - val_accuracy: 0.9941 - val_loss: 0.1064\n",
      "Epoch 13/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9958 - loss: 0.1046 - val_accuracy: 0.9950 - val_loss: 0.1080\n",
      "Epoch 14/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9952 - loss: 0.1052 - val_accuracy: 0.9938 - val_loss: 0.1078\n",
      "Epoch 15/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.1042 - val_accuracy: 0.9948 - val_loss: 0.1091\n",
      "Epoch 16/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.1045 - val_accuracy: 0.9940 - val_loss: 0.1093\n",
      "Epoch 17/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9952 - loss: 0.1045 - val_accuracy: 0.9940 - val_loss: 0.1072\n",
      "Epoch 18/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.1042 - val_accuracy: 0.9957 - val_loss: 0.1091\n",
      "Epoch 19/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.1052 - val_accuracy: 0.9948 - val_loss: 0.1079\n",
      "Epoch 20/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.1046 - val_accuracy: 0.9948 - val_loss: 0.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: G:\\My Drive\\LABS\\Experimental Linguistics lab\\retraining csvs\\pitch\\Pitch_LSTM_model.h5\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.9964 - loss: 0.1059\n",
      "Test Loss: 0.10589616745710373, Test Accuracy: 0.996417224407196\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Prominence       0.91      0.96      0.93      4393\n",
      "    Boundary       0.51      0.86      0.64       190\n",
      "\n",
      "   micro avg       0.89      0.95      0.92      4583\n",
      "   macro avg       0.71      0.91      0.79      4583\n",
      "weighted avg       0.90      0.95      0.92      4583\n",
      " samples avg       0.57      0.58      0.57      4583\n",
      "\n",
      "Combined Precision: 0.8865, Combined Recall: 0.9511, Combined F1-Score: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load all CSV files\n",
    "folder_path = r\"G:\\Shared drives\\Prosody\\RPT model\\pitch\"\n",
    "file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Combine data from all CSV files\n",
    "data = []\n",
    "annotations = []\n",
    "\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file, header=0)  # Assumes the first row is the header\n",
    "\n",
    "    # The columns for the features in the CSVs are:\n",
    "    # Min, Max, Mean, Standard Deviation, Z-Score, Duration, POS IDs\n",
    "    features = df.iloc[:, [2, 3, 4, 5, 6, 9, 10, ]].values  # Select columns C, D, E, F, G, J\n",
    "\n",
    "    # The columns for the labels are Prominence, and Boundary\n",
    "    labels = df.iloc[:, [11, 12]].values  # Select columns K, L\n",
    "\n",
    "    data.append(features)\n",
    "    annotations.append(labels)\n",
    "\n",
    "# Flatten the data into arrays\n",
    "data = np.vstack(data)\n",
    "annotations = np.vstack(annotations)\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data for LSTM\n",
    "time_steps = 1  # Adjust if needed for sequences\n",
    "X_train = X_train.reshape((X_train.shape[0], time_steps, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], time_steps, X_test.shape[1]))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(time_steps, X_train.shape[2])),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(annotations.shape[1], activation='sigmoid')  # Use sigmoid for multi-label binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Define the model save path\n",
    "model_save_path = os.path.join(folder_path, \"Pitch_LSTM_model.h5\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "#y_pred = (model.predict(X_test) > 0.4).astype(int)\n",
    "\n",
    "# Calculate precision, recall, F1-score\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_test, y_pred, target_names=[f\"Class {i}\" for i in range(annotations.shape[1])]))\n",
    "\n",
    "# Compute and display combined scores\n",
    "#precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "#print(f\"Combined Precision: {precision:.4f}, Combined Recall: {recall:.4f}, Combined F1-Score: {f1:.4f}\")\n",
    "# Make predictions\n",
    "y_pred_raw = model.predict(X_test)\n",
    "\n",
    "# Apply different thresholds\n",
    "y_pred = np.zeros_like(y_pred_raw)\n",
    "y_pred[:, 0] = (y_pred_raw[:, 0] > 0.4).astype(int)  # Prominence threshold\n",
    "y_pred[:, 1] = (y_pred_raw[:, 1] > 0.16).astype(int)  # Boundary threshold\n",
    "\n",
    "# Calculate precision, recall, F1-score\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Prominence\", \"Boundary\"]))\n",
    "\n",
    "# Compute and display combined scores\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print(f\"Combined Precision: {precision:.4f}, Combined Recall: {recall:.4f}, Combined F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cbfe5",
   "metadata": {},
   "source": [
    "# Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38537b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.3182 - val_accuracy: 0.9883 - val_loss: 0.1894\n",
      "Epoch 2/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1702 - val_accuracy: 0.9269 - val_loss: 0.1641\n",
      "Epoch 3/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9175 - loss: 0.1590 - val_accuracy: 0.9082 - val_loss: 0.1576\n",
      "Epoch 4/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9014 - loss: 0.1537 - val_accuracy: 0.9187 - val_loss: 0.1528\n",
      "Epoch 5/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9026 - loss: 0.1501 - val_accuracy: 0.9035 - val_loss: 0.1489\n",
      "Epoch 6/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8956 - loss: 0.1462 - val_accuracy: 0.9260 - val_loss: 0.1580\n",
      "Epoch 7/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.1441 - val_accuracy: 0.8890 - val_loss: 0.1616\n",
      "Epoch 8/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9033 - loss: 0.1414 - val_accuracy: 0.9086 - val_loss: 0.1392\n",
      "Epoch 9/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9052 - loss: 0.1389 - val_accuracy: 0.9187 - val_loss: 0.1376\n",
      "Epoch 10/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9089 - loss: 0.1362 - val_accuracy: 0.9106 - val_loss: 0.1509\n",
      "Epoch 11/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9098 - loss: 0.1347 - val_accuracy: 0.9038 - val_loss: 0.1365\n",
      "Epoch 12/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9057 - loss: 0.1338 - val_accuracy: 0.9119 - val_loss: 0.1337\n",
      "Epoch 13/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9126 - loss: 0.1330 - val_accuracy: 0.9187 - val_loss: 0.1358\n",
      "Epoch 14/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9111 - loss: 0.1320 - val_accuracy: 0.9093 - val_loss: 0.1305\n",
      "Epoch 15/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9120 - loss: 0.1327 - val_accuracy: 0.9066 - val_loss: 0.1355\n",
      "Epoch 16/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9107 - loss: 0.1317 - val_accuracy: 0.9236 - val_loss: 0.1333\n",
      "Epoch 17/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9143 - loss: 0.1322 - val_accuracy: 0.8910 - val_loss: 0.1353\n",
      "Epoch 18/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9131 - loss: 0.1309 - val_accuracy: 0.9176 - val_loss: 0.1353\n",
      "Epoch 19/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9115 - loss: 0.1316 - val_accuracy: 0.9249 - val_loss: 0.1337\n",
      "Epoch 20/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9136 - loss: 0.1316 - val_accuracy: 0.9104 - val_loss: 0.1304\n",
      "Epoch 1/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9127 - loss: 0.1310 - val_accuracy: 0.9178 - val_loss: 0.1303\n",
      "Epoch 2/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9142 - loss: 0.1305 - val_accuracy: 0.9280 - val_loss: 0.1304\n",
      "Epoch 3/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9176 - loss: 0.1303 - val_accuracy: 0.9284 - val_loss: 0.1320\n",
      "Epoch 4/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9193 - loss: 0.1308 - val_accuracy: 0.9110 - val_loss: 0.1376\n",
      "Epoch 5/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9205 - loss: 0.1300 - val_accuracy: 0.9128 - val_loss: 0.1307\n",
      "Epoch 6/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9197 - loss: 0.1304 - val_accuracy: 0.9245 - val_loss: 0.1469\n",
      "Epoch 7/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9247 - loss: 0.1306 - val_accuracy: 0.9114 - val_loss: 0.1312\n",
      "Epoch 8/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9228 - loss: 0.1301 - val_accuracy: 0.9392 - val_loss: 0.1321\n",
      "Epoch 9/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9227 - loss: 0.1312 - val_accuracy: 0.9092 - val_loss: 0.1511\n",
      "Epoch 10/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9273 - loss: 0.1304 - val_accuracy: 0.9289 - val_loss: 0.1298\n",
      "Epoch 11/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9305 - loss: 0.1292 - val_accuracy: 0.9247 - val_loss: 0.1296\n",
      "Epoch 12/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9243 - loss: 0.1299 - val_accuracy: 0.9141 - val_loss: 0.1335\n",
      "Epoch 13/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9270 - loss: 0.1299 - val_accuracy: 0.9313 - val_loss: 0.1298\n",
      "Epoch 14/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9301 - loss: 0.1291 - val_accuracy: 0.9073 - val_loss: 0.1335\n",
      "Epoch 15/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9276 - loss: 0.1299 - val_accuracy: 0.9383 - val_loss: 0.1318\n",
      "Epoch 16/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9326 - loss: 0.1290 - val_accuracy: 0.9236 - val_loss: 0.1301\n",
      "Epoch 17/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9300 - loss: 0.1296 - val_accuracy: 0.9368 - val_loss: 0.1316\n",
      "Epoch 18/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9328 - loss: 0.1295 - val_accuracy: 0.9390 - val_loss: 0.1363\n",
      "Epoch 19/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9338 - loss: 0.1293 - val_accuracy: 0.9407 - val_loss: 0.1338\n",
      "Epoch 20/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9331 - loss: 0.1295 - val_accuracy: 0.9500 - val_loss: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: G:\\My Drive\\LABS\\Experimental Linguistics lab\\retraining csvs\\intensity\\Intensity_LSTM_model.h5\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9451 - loss: 0.1253\n",
      "Test Loss: 0.12533189356327057, Test Accuracy: 0.9450549483299255\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Prominence       0.95      0.95      0.95      3585\n",
      "    Boundary       0.58      0.79      0.67       558\n",
      "\n",
      "   micro avg       0.88      0.93      0.90      4143\n",
      "   macro avg       0.76      0.87      0.81      4143\n",
      "weighted avg       0.90      0.93      0.91      4143\n",
      " samples avg       0.48      0.50      0.48      4143\n",
      "\n",
      "Combined Precision: 0.8827, Combined Recall: 0.9281, Combined F1-Score: 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load all CSV files\n",
    "folder_path = r\"G:\\Shared drives\\Prosody\\RPT model\\intensity\"\n",
    "file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Combine data from all CSV files\n",
    "data = []\n",
    "annotations = []\n",
    "\n",
    "for file in file_list:\n",
    "\n",
    "    # The columns for the features in the CSVs are:\n",
    "    # Min, Max, Mean, Standard Deviation, Z-Score, Duration, POS IDs\n",
    "    df = pd.read_csv(file, header=0)  # Assumes the first row is the header\n",
    "    features = df.iloc[:, [2, 3, 4, 5, 6, 9, 10, ]].values  # Select columns C, D, E, F, G, J\n",
    "\n",
    "    # The columns for the labels are Prominence, and Boundary\n",
    "    labels = df.iloc[:, [11, 12]].values  # Select columns K, L\n",
    "\n",
    "    data.append(features)\n",
    "    annotations.append(labels)\n",
    "\n",
    "# Flatten the data into arrays\n",
    "data = np.vstack(data)\n",
    "annotations = np.vstack(annotations)\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data for LSTM\n",
    "time_steps = 1  # Adjust if needed for sequences\n",
    "X_train = X_train.reshape((X_train.shape[0], time_steps, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], time_steps, X_test.shape[1]))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(time_steps, X_train.shape[2])),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(annotations.shape[1], activation='sigmoid')  # Use sigmoid for multi-label binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Define the model save path\n",
    "model_save_path = os.path.join(folder_path, \"Intensity_LSTM_model.h5\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred_raw = model.predict(X_test)\n",
    "\n",
    "# Apply different thresholds\n",
    "y_pred = np.zeros_like(y_pred_raw)\n",
    "y_pred[:, 0] = (y_pred_raw[:, 0] > 0.4).astype(int)  # Prominence threshold\n",
    "y_pred[:, 1] = (y_pred_raw[:, 1] > 0.16).astype(int)  # Boundary threshold\n",
    "\n",
    "# Calculate precision, recall, F1-score\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Prominence\", \"Boundary\"]))\n",
    "\n",
    "# Compute and display combined scores\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print(f\"Combined Precision: {precision:.4f}, Combined Recall: {recall:.4f}, Combined F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfd3d5",
   "metadata": {},
   "source": [
    "# 2CSV Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.3315 - val_accuracy: 0.9733 - val_loss: 0.1919\n",
      "Epoch 2/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9422 - loss: 0.1695 - val_accuracy: 0.8912 - val_loss: 0.1696\n",
      "Epoch 3/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8931 - loss: 0.1524 - val_accuracy: 0.9048 - val_loss: 0.1514\n",
      "Epoch 4/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8867 - loss: 0.1452 - val_accuracy: 0.8930 - val_loss: 0.1447\n",
      "Epoch 5/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8792 - loss: 0.1412 - val_accuracy: 0.8711 - val_loss: 0.1444\n",
      "Epoch 6/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.1387 - val_accuracy: 0.8877 - val_loss: 0.1474\n",
      "Epoch 7/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8741 - loss: 0.1368 - val_accuracy: 0.8588 - val_loss: 0.1404\n",
      "Epoch 8/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8745 - loss: 0.1362 - val_accuracy: 0.8778 - val_loss: 0.1361\n",
      "Epoch 9/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8780 - loss: 0.1345 - val_accuracy: 0.8745 - val_loss: 0.1366\n",
      "Epoch 10/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8822 - loss: 0.1338 - val_accuracy: 0.8727 - val_loss: 0.1363\n",
      "Epoch 11/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8814 - loss: 0.1331 - val_accuracy: 0.8771 - val_loss: 0.1364\n",
      "Epoch 12/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8836 - loss: 0.1329 - val_accuracy: 0.8824 - val_loss: 0.1349\n",
      "Epoch 13/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8869 - loss: 0.1332 - val_accuracy: 0.8930 - val_loss: 0.1352\n",
      "Epoch 14/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8893 - loss: 0.1330 - val_accuracy: 0.8813 - val_loss: 0.1338\n",
      "Epoch 15/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8924 - loss: 0.1328 - val_accuracy: 0.9053 - val_loss: 0.1350\n",
      "Epoch 16/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.1315 - val_accuracy: 0.8976 - val_loss: 0.1335\n",
      "Epoch 17/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9036 - loss: 0.1321 - val_accuracy: 0.9011 - val_loss: 0.1319\n",
      "Epoch 18/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9043 - loss: 0.1317 - val_accuracy: 0.9024 - val_loss: 0.1358\n",
      "Epoch 19/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9070 - loss: 0.1318 - val_accuracy: 0.9046 - val_loss: 0.1327\n",
      "Epoch 20/20\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9123 - loss: 0.1313 - val_accuracy: 0.9082 - val_loss: 0.1389\n",
      "Epoch 1/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9907 - loss: 0.2957 - val_accuracy: 0.9881 - val_loss: 0.1655\n",
      "Epoch 2/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.1406 - val_accuracy: 0.9860 - val_loss: 0.1272\n",
      "Epoch 3/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.1266 - val_accuracy: 0.9792 - val_loss: 0.1348\n",
      "Epoch 4/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9829 - loss: 0.1229 - val_accuracy: 0.9786 - val_loss: 0.1264\n",
      "Epoch 5/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9857 - loss: 0.1202 - val_accuracy: 0.9859 - val_loss: 0.1184\n",
      "Epoch 6/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9880 - loss: 0.1182 - val_accuracy: 0.9900 - val_loss: 0.1155\n",
      "Epoch 7/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9959 - loss: 0.1165 - val_accuracy: 0.9957 - val_loss: 0.1189\n",
      "Epoch 8/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.1161 - val_accuracy: 0.9976 - val_loss: 0.1151\n",
      "Epoch 9/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.1147 - val_accuracy: 0.9978 - val_loss: 0.1163\n",
      "Epoch 10/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.1145 - val_accuracy: 0.9976 - val_loss: 0.1219\n",
      "Epoch 11/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.1152 - val_accuracy: 0.9983 - val_loss: 0.1169\n",
      "Epoch 12/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.1133 - val_accuracy: 0.9979 - val_loss: 0.1141\n",
      "Epoch 13/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.1136 - val_accuracy: 0.9974 - val_loss: 0.1130\n",
      "Epoch 14/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.1126 - val_accuracy: 0.9981 - val_loss: 0.1116\n",
      "Epoch 15/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.1111 - val_accuracy: 0.9976 - val_loss: 0.1109\n",
      "Epoch 16/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.1112 - val_accuracy: 0.9981 - val_loss: 0.1132\n",
      "Epoch 17/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.1109 - val_accuracy: 0.9976 - val_loss: 0.1094\n",
      "Epoch 18/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.1108 - val_accuracy: 0.9979 - val_loss: 0.1102\n",
      "Epoch 19/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.1095 - val_accuracy: 0.9983 - val_loss: 0.1098\n",
      "Epoch 20/20\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.1111 - val_accuracy: 0.9972 - val_loss: 0.1100\n",
      "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Prominence       0.53      1.00      0.69      3585\n",
      "    Boundary       0.08      1.00      0.15       558\n",
      "\n",
      "   micro avg       0.30      1.00      0.47      4143\n",
      "   macro avg       0.30      1.00      0.42      4143\n",
      "weighted avg       0.47      1.00      0.62      4143\n",
      " samples avg       0.30      0.53      0.38      4143\n",
      "\n",
      "Combined Precision: 0.3035, Combined Recall: 1.0000, Combined F1-Score: 0.4657\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Load and process intensity data\n",
    "def load_data(folder_path):\n",
    "    file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "    data = []\n",
    "    annotations = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        features = df.iloc[:, [2, 3, 4, 5, 6, 9, 10]].values  # Select columns C, D, E, F, G, J\n",
    "        labels = df.iloc[:, [11, 12]].values  # Select columns K, L\n",
    "        data.append(features)\n",
    "        annotations.append(labels)\n",
    "    data = np.vstack(data)\n",
    "    annotations = np.vstack(annotations)\n",
    "    return data, annotations\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=input_shape),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(output_shape, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load intensity and pitch data\n",
    "folder_path_intensity = r\"G:\\Shared drives\\Prosody\\RPT model\\intensity\"\n",
    "folder_path_pitch = r\"G:\\Shared drives\\Prosody\\RPT model\\pitch\"\n",
    "\n",
    "intensity_data, intensity_annotations = load_data(folder_path_intensity)\n",
    "pitch_data, pitch_annotations = load_data(folder_path_pitch)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "intensity_data = scaler.fit_transform(intensity_data)\n",
    "pitch_data = scaler.transform(pitch_data)  # Use the same scaler for both\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train_intensity, X_test_intensity, y_train_intensity, y_test_intensity = train_test_split(\n",
    "    intensity_data, intensity_annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_pitch, X_test_pitch, y_train_pitch, y_test_pitch = train_test_split(\n",
    "    pitch_data, pitch_annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape data for LSTM\n",
    "time_steps = 1\n",
    "X_train_intensity = X_train_intensity.reshape((X_train_intensity.shape[0], time_steps, X_train_intensity.shape[1]))\n",
    "X_test_intensity = X_test_intensity.reshape((X_test_intensity.shape[0], time_steps, X_test_intensity.shape[1]))\n",
    "X_train_pitch = X_train_pitch.reshape((X_train_pitch.shape[0], time_steps, X_train_pitch.shape[1]))\n",
    "X_test_pitch = X_test_pitch.reshape((X_test_pitch.shape[0], time_steps, X_test_pitch.shape[1]))\n",
    "\n",
    "# Create models\n",
    "intensity_model = create_lstm_model((time_steps, X_train_intensity.shape[2]), y_train_intensity.shape[1])\n",
    "pitch_model = create_lstm_model((time_steps, X_train_pitch.shape[2]), y_train_pitch.shape[1])\n",
    "\n",
    "# Train the models\n",
    "intensity_model.fit(X_train_intensity, y_train_intensity, epochs=20, batch_size=32, validation_split=0.2)\n",
    "pitch_model.fit(X_train_pitch, y_train_pitch, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Make predictions (raw output/logits)\n",
    "y_pred_intensity_raw = intensity_model.predict(X_test_intensity)\n",
    "y_pred_pitch_raw = pitch_model.predict(X_test_pitch)\n",
    "\n",
    "# Interpolation to match the lengths of both prediction arrays\n",
    "def interpolate_predictions(predictions, target_length):\n",
    "    # Create a linear interpolation function\n",
    "    x = np.linspace(0, len(predictions) - 1, len(predictions))\n",
    "    f = interp1d(x, predictions, axis=0, fill_value=\"extrapolate\")\n",
    "    x_new = np.linspace(0, len(predictions) - 1, target_length)\n",
    "    return f(x_new)\n",
    "\n",
    "# Determine the max length between the two sets of predictions\n",
    "max_length = max(len(y_pred_intensity_raw), len(y_pred_pitch_raw))\n",
    "\n",
    "# Interpolate predictions from both models to the same length\n",
    "y_pred_intensity_interpolated = interpolate_predictions(y_pred_intensity_raw, max_length)\n",
    "y_pred_pitch_interpolated = interpolate_predictions(y_pred_pitch_raw, max_length)\n",
    "\n",
    "# Step 1: Combine the raw predictions using simple averaging (you could apply weighted averaging if needed)\n",
    "y_pred_combined_raw = (y_pred_intensity_interpolated + y_pred_pitch_interpolated) / 2\n",
    "\n",
    "# Step 2: Apply sigmoid activation to the combined raw predictions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "y_pred_combined_prob = sigmoid(y_pred_combined_raw)\n",
    "\n",
    "# Step 3: Apply thresholds for each class\n",
    "y_pred_combined_final = np.zeros_like(y_pred_combined_prob)\n",
    "\n",
    "# Apply different thresholds for each class\n",
    "# Prominence (class 0) with a higher threshold (e.g., 0.6)\n",
    "y_pred_combined_final[:, 0] = (y_pred_combined_prob[:, 0] > 0.5).astype(int)\n",
    "\n",
    "# Boundary (class 1) with a lower threshold (e.g., 0.4)\n",
    "y_pred_combined_final[:, 1] = (y_pred_combined_prob[:, 1] > 0.4).astype(int)\n",
    "\n",
    "# Ensure the length of predictions matches the length of ground truth\n",
    "min_length = min(len(y_test_intensity), len(y_pred_combined_final))\n",
    "y_test_intensity = y_test_intensity[:min_length]\n",
    "y_pred_combined_final = y_pred_combined_final[:min_length]\n",
    "\n",
    "# Now calculate the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_intensity, y_pred_combined_final, target_names=[\"Prominence\", \"Boundary\"]))\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_intensity, y_pred_combined_final, average='micro')\n",
    "print(f\"Combined Precision: {precision:.4f}, Combined Recall: {recall:.4f}, Combined F1-Score: {f1:.4f}\")\n",
    "\n",
    "combined_folder_path = r\"G:\\Shared drives\\Prosody\\RPT model\" # change as needed\n",
    "model.save(os.path.join(combined_folder_path, \"Combined_LSTM_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931bd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
